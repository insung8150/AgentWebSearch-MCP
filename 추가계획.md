──────────────────────────────────────────────────────────────────────────────────────────────────
❯ 그래 우리가 하고싶은거는로컬llm을 돌릴때 검색이 어려운것을 쉽게 하자는것 아니니 근데            
  근본적으로 사용자는 로컬llm을 쓴다는것이고 또는 외부 저가 llm이지만 websearch기능이 없는것을    
  사용할수있도록 하기위함이 아니니 결국은 llm이 있어 그것이 성능이 좋든나쁘든 근데 우리가         
  만든것은 이 것에 특화된 4b모델을 사용하면 제일좋은데 gpu낭비가 심해져서 그래서 사용자자 자기가  
  쓰는 모델이랑 이것을위한 모델이랑 결합하고 싶어 할꺼라는거지 이에 대비해서 모델을 선택해서      
  사용할수 있도록 해야할것 같아 이해해?       


   그런대 지금 sglang 와 서버에서 다른 lmstudio등을 붙이면 데이터 호환이 안된다고 하였음.  

                                                                                                    
● 맞아요! 이전에 논의한 내용:                                                                     
                                                                                                  
  형식 차이:                                                                                      
  ┌─────────────────────────┬───────────────────────────────────────────────────────┐             
  │         백엔드          │                    도구 호출 형식                     │             
  ├─────────────────────────┼───────────────────────────────────────────────────────┤             
  │ AgentCPM-Explore        │ <tool_call>{"name": "search"...}</tool_call> (텍스트) │             
  ├─────────────────────────┼───────────────────────────────────────────────────────┤             
  │ Ollama/LM Studio/OpenAI │ tools API 파라미터 (JSON)                             │             
  └─────────────────────────┴───────────────────────────────────────────────────────┘             
  호환 안 되는 이유:                                                                              
  - AgentCPM-Explore = 텍스트 기반 <tool_call> 태그로 훈련됨                                      
  - 다른 모델들 = OpenAI function calling API 형식                                                
                                                                                                  
  해결책: 어댑터 레이어                                                                           
  [search_agent.py]                                                                               
         ↓                                                                                        
  [LLM Adapter] ← 백엔드별 형식 변환                                                              
    ├── sglang_adapter.py   → <tool_call> 파싱                                                    
    ├── ollama_adapter.py   → function calling API                                                
    ├── lmstudio_adapter.py → function calling API                                                
    └── openai_adapter.py   → function calling API                                                
         ↓                                                                                        
  [CDP Search]                                                                                    
                                                                                                  
  각 어댑터가:                                                                                    
  - 해당 백엔드에 맞는 프롬프트/API 형식 사용                                                     
  - 응답을 통일된 형식으로 변환                                                                   
                                                                                                  
  이 어댑터 레이어를 추가해야 다양한 LLM 지원이 가능합니다. 진행할까요?                           
                                                                                                         
❯ 이것이 mcp로 만들어졌을때 이부분을 어떻게 해서 선택하도록 하지?                                 
                                                                                                                                              
❯ 그것이 이프로젝트의 목적이 아니라니까 .  검색어를 넣으면 llm이 연관추천검갯어를 만들고 검색하고 요약을        
  해서 상위 llm에서 던져 주는것이 목적이라니가? 그래야 항상 너의  websearch처럼 스마트한 검색이                 
  이루어지쟎야                                                                                                                                                                                           
● 아! 이제 완전히 이해했습니다!                                                                                 
                                                                                                                
  목적:                                                                                                         
  [상위 LLM (사용자의 메인 LLM)]                                                                                
          ↓ "GPT-5 출시일 알려줘"                                                                               
  [LocalWebSearch MCP + 내장 검색 LLM]                                                                          
    ├── 연관 검색어 생성 (LLM)                                                                                  
    ├── 병렬 검색 (CDP)                                                                                         
    └── 결과 요약 (LLM)                                                                                         
          ↓ "GPT-5는 2025년 8월 출시됨..." (요약된 결과)                                                        
  [상위 LLM이 최종 답변 생성]                                                                                   
                                                                                                                
  = Claude WebSearch와 동일한 방식!                                                                             
                                                                                                                
  MCP 설정에서 내장 LLM 선택:                                                                                   
  {                                                                                                             
    "mcpServers": {                                                                                             
      "localwebsearch": {                                                                                       
        "command": "python",                                                                                    
        "args": ["mcp_server.py"],                                                                              
        "env": {                                                                                                
          "LLM_BACKEND": "ollama",                                                                              
          "LLM_MODEL": "qwen2.5:7b"                                                                             
        }                                                                                                       
      }                                                                                                         
    }                                                                                                           
  }                                                                                                             
                                                                                                                
  지원 백엔드:                                                                                                  
  ┌──────────────────────┬──────────────────────────┐                                                           
  │       환경변수       │           예시           │                                                           
  ├──────────────────────┼──────────────────────────┤                                                           
  │ LLM_BACKEND=sglang   │ LLM_MODEL=30001          │                                                           
  ├──────────────────────┼──────────────────────────┤                                                           
  │ LLM_BACKEND=ollama   │ LLM_MODEL=qwen2.5:7b     │                                                           
  ├──────────────────────┼──────────────────────────┤                                                           
  │ LLM_BACKEND=lmstudio │ LLM_MODEL=localhost:1234 │                                                           
  ├──────────────────────┼──────────────────────────┤                                                           
  │ LLM_BACKEND=openai   │ LLM_MODEL=gpt-4o-mini    │                                                           
  └──────────────────────┴──────────────────────────┘                                                           
  이 방향 맞죠?                                                                                                 
                                                                                                                                                 
● 좋은 질문! MCP에서 LLM 선택 방법:                                                                             
                                                                                                                
  옵션 1: 환경변수 (설치 시 고정)                                                                               
  {                                                                                                             
    "env": {                                                                                                    
      "LLM_BACKEND": "ollama",                                                                                  
      "LLM_MODEL": "qwen2.5:7b"                                                                                 
    }                                                                                                           
  }                                                                                                             
  - 단점: 바꾸려면 MCP 재시작                                                                                   
                                                                                                                
  옵션 2: 설정 파일                                                                                             
  ~/.config/localwebsearch/config.json                                                                          
  - 단점: 역시 재시작 필요                                                                                      
                                                                                                                
  옵션 3: MCP 도구로 제공                                                                                       
  # MCP가 제공하는 도구                                                                                         
  tools = [                                                                                                     
      "search(query)",                                                                                          
      "fetch_url(url)",                                                                                         
      "set_llm(backend, model)",  # ← 런타임 변경!                                                              
      "list_available_llms()",     # ← 사용 가능한 LLM 목록                                                     
  ]                                                                                                             
  - 장점: 동적 변경 가능                                                                                        
                                                                                                                
  옵션 4: 자동 감지 (추천)                                                                                      
  MCP 서버 시작 시:                                                                                             
  1. SGLang 실행 중? → 사용                                                                                     
  2. LM Studio 실행 중? → 사용                                                                                  
  3. Ollama 실행 중? → 사용                                                                                     
  4. 없으면? → OpenAI 또는 에러                                                                                 
                                                                                                                
  조합 추천:                                                                                                    
  - 기본: 자동 감지                                                                                             
  - 오버라이드: 환경변수                                                                                        
  - 런타임 변경: set_llm 도구                                                                                   
                                                                                                                
  이 방향 어때요?                                      
   ❯ mcp 가 동작하면 이것을 자동체크해서 사용자에게 선택을 하라고 한뒤 선택되어지었으면 진행하고록 하고는것이      
  어떤가?  그 세션에서는 한번 선택되어지면 계속 그대로 사용하고 변경사황이 생기면 추가 선택하도록 하는것이      
  어떤가?             